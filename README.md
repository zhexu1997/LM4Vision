# LM4Vision

## Surveys
+ Towards Rationality in Language and Multimodal Agents: A Survey, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2406.00252) [[Project]](https://github.com/bowen-upenn/Agent_Rationality)
+ Large Multimodal Agents: A Survey, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2402.15116) [[Project]](https://github.com/jun0wanan/awesome-large-multimodal-agents)
+ Large Language Models on Graphs: A Comprehensive Survey, TKDE 2024. [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10697304&casa_token=naDG0JEQSlwAAAAA:0oLETJgnMk59KDDZy4vVH5uK7-MuLn0y-mJ9ZPqZBu1d1TqRU6sfXImNn5OZ9c6WzL8egHrBoPgt) [[Project]](https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs)
+ A survey on large language model based autonomous agents, Frontiers of Computer Science 2024. [[Paper]](https://link.springer.com/content/pdf/10.1007/s11704-024-40231-1.pdf)
+ A comprehensive overview of large language models, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2307.06435)
+ Towards Data-and Knowledge-Driven AI: A Survey on Neuro-Symbolic Computing, TPAMI 2024. [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721277&casa_token=_uyrw_egIvoAAAAA:j2nu6aF-UeP9b93aQppDGlucteehGyN-ow0B0O8YRuOMVIIAOzENJmevPBeluy956pesmFzN_Q&tag=1)
+ A review on large Language Models: Architectures, applications, taxonomies, open issues and challenges, Access 2024. [[Paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10433480)
+ Large Language Model based Multi-Agents: A Survey of Progress and Challenges, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2402.01680)
+ Video Understanding with Large Language Models: A Survey, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2312.17432) [[Code]](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)
## Papers
### Image Understanding
+ Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Self-Training_Large_Language_Models_for_Improved_Visual_Program_Synthesis_With_CVPR_2024_paper.pdf) [[Code]](https://zaidkhan.me/ViReP/)
+ PixelLM: Pixel Reasoning with Large Multimodal Model, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Ren_PixelLM_Pixel_Reasoning_with_Large_Multimodal_Model_CVPR_2024_paper.pdf) [[Code]](https://pixellm.github.io/)
+ Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Xuan_Pink_Unveiling_the_Power_of_Referential_Comprehension_for_Multi-modal_LLMs_CVPR_2024_paper.pdf) [[Code]](https://github.com/SY-Xuan/Pink)
+ Large Language Models are Visual Reasoning Coordinators, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdf) [[Code]](https://github.com/cliangyu/Cola)
+ Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf) [[Code]](https://chameleon-llm.github.io/)
+ Visual Instruction Tuning, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf) [[Code]](https://llava-vl.github.io/)
+ Vipergpt: Visual inference via python execution for reasoning, ICCV 2023. [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf)  [[Code]](https://github.com/cvlab-columbia/viper)
+ Visual programming: Compositional visual reasoning without training, CVPR 2023. [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf) [[Code]](https://prior.allenai.org/projects/visprog)
### Video Understanding
+ DrVideo: Document Retrieval Based Long Video Understanding, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2406.12846) 
+ VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.11481) [[Code]](https://videoagent.github.io/)
+ VideoAgent: Long-form Video Understanding with Large Language Model as Agent, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.10517) [[Code]](https://github.com/wxh1996/VideoAgent)
+ VideoChat: Chat-Centric Video Understanding, CVPR 2024. [[Paper]](https://arxiv.org/pdf/2305.06355) [[Code]](https://github.com/OpenGVLab/Ask-Anything)
+ DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent), ICML 2024. [[Paper]](https://arxiv.org/pdf/2401.08392) [[Code]](https://github.com/z-x-yang/DoraemonGPT)
+ InternVideo2: Scaling Foundation Models for Multimodal Video Understanding, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.15377) [[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)
+ InternVideo: General Video Foundation Models via Generative and Discriminative Learning, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2212.03191) [[Code]](https://github.com/OpenGVLab/InternVideo)
+ VTimeLLM: Empower LLM to Grasp Video Moments, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_VTimeLLM_Empower_LLM_to_Grasp_Video_Moments_CVPR_2024_paper.pdf) [[Code]](https://github.com/huangb23/VTimeLLM)
+ ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2304.14407) [[Code]](https://www.wangjunke.info/ChatVideo/)
+ AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2306.08640) [[Code]](https://github.com/showlab/assistgpt)
### Image&Video Understanding
+ Chat-univi: Unified visual representation empowers large language models with image and video understanding, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.pdf) [[Code]](https://github.com/PKU-YuanGroup/Chat-UniVi)
