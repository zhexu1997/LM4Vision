# LM4Vision

## Surveys
+ Towards Rationality in Language and Multimodal Agents: A Survey, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2406.00252) [[Project]](https://github.com/bowen-upenn/Agent_Rationality)
+ Large Multimodal Agents: A Survey, arXiv 2024. [[Paper]](https://arxiv.org/pdf/2402.15116) [[Project]](https://github.com/jun0wanan/awesome-large-multimodal-agents)
+ A survey on large language model based autonomous agents, Frontiers of Computer Science 2024. [[Paper]](https://link.springer.com/content/pdf/10.1007/s11704-024-40231-1.pdf)
+ A comprehensive overview of large language models, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2307.06435)
## Papers
### Image Understanding
+ Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Khan_Self-Training_Large_Language_Models_for_Improved_Visual_Program_Synthesis_With_CVPR_2024_paper.pdf) [[Code]](https://zaidkhan.me/ViReP/)
+ Large Language Models are Visual Reasoning Coordinators, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdf) [[Code]](https://github.com/cliangyu/Cola)
+ Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/871ed095b734818cfba48db6aeb25a62-Paper-Conference.pdf) [[Code]](https://chameleon-llm.github.io/)
+ Visual Instruction Tuning, NeurIPS 2023. [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf) [[Code]](https://llava-vl.github.io/)
+ Vipergpt: Visual inference via python execution for reasoning, ICCV 2023. [[Paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf) 
### Video Understanding
+ VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.11481) [[Code]](https://videoagent.github.io/)
+ VideoAgent: Long-form Video Understanding with Large Language Model as Agent, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.10517) [[Code]](https://github.com/wxh1996/VideoAgent)
+ DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent), ICML 2024. [[Paper]](https://arxiv.org/pdf/2401.08392)
+ InternVideo2: Scaling Foundation Models for Multimodal Video Understanding, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2403.15377) [[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)
+ InternVideo: General Video Foundation Models via Generative and Discriminative Learning, ECCV 2024. [[Paper]](https://arxiv.org/pdf/2212.03191) [[Code]](https://github.com/OpenGVLab/InternVideo)
+ ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2304.14407) [[Code]](https://www.wangjunke.info/ChatVideo/)
+ AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn, arXiv 2023. [[Paper]](https://arxiv.org/pdf/2306.08640) [[Code]](https://github.com/showlab/assistgpt)
### Image&Video Understanding
+ Chat-univi: Unified visual representation empowers large language models with image and video understanding, CVPR 2024. [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Jin_Chat-UniVi_Unified_Visual_Representation_Empowers_Large_Language_Models_with_Image_CVPR_2024_paper.pdf) [[Code]](https://github.com/PKU-YuanGroup/Chat-UniVi)
